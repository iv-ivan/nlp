{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500          0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df.shape, df.is_blocked.mean()\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print \"Blocked ratio\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.5\n",
      "Count: 549992\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "\n",
    "#df = df[df.is_blocked == 1][np.random.permutation(len(df[df.is_blocked == 1]))] + df[df.is_blocked == 0]\n",
    "df = pd.concat([df[df.is_blocked == 1], df[df.is_blocked == 0].sample(len(df[df.is_blocked == 1]))])\n",
    "\n",
    "print \"Blocked ratio:\",df.is_blocked.mean()\n",
    "print \"Count:\",len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print \"All tests passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.decode('utf8').lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZmhIkrr9fxTJs586a3qKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1370baad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(token_counts.values(),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "new_tokens = Counter()\n",
    "min_count = 10\n",
    "for k, v in token_counts.iteritems():\n",
    "    if v >= min_count:\n",
    "        new_tokens[k] = v\n",
    "\n",
    "tokens = new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87870\n"
     ]
    }
   ],
   "source": [
    "print \"# Tokens:\",len(token_to_id)\n",
    "if len(token_to_id) < 30000:\n",
    "    print \"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\"\n",
    "if len(token_to_id) > 1000000:\n",
    "    print \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.decode('utf8').lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = map(lambda token: token_to_id.get(token,0), tokens)[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (549992, 20)\n",
      "Поездки на таможню, печать в паспорте -> [67639 58481 45108  6607 55888 68946     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [32937     0 33339     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [27044 44641     0 14262 52267     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Размер матрицы:\",title_tokens.shape\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print title,'->', tokens[:10],'...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":x[0], \"subcategory\":x[1]} for x in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390398  29431 205783  15521   5163  66893 182059 212079  94302 215368] [299777 431001 122518 318297 165012 252836 114140 511698 497226 218435]\n"
     ]
    }
   ],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard \n",
    "perm = np.random.permutation(len(title_tokens))\n",
    "tr_ind = perm[:len(perm)*4/5]\n",
    "ts_ind = perm[len(perm)*4/5:]\n",
    "print tr_ind[:10], ts_ind[:10]\n",
    "\n",
    "data_tuple = title_tokens[tr_ind],title_tokens[ts_ind],desc_tokens[tr_ind],desc_tokens[ts_ind],np.array(df_non_text)[tr_ind],np.array(df_non_text)[ts_ind],target[tr_ind],target[ts_ind]\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving preprocessed data (may take up to 3 minutes)\n",
      "готово\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print \"Saving preprocessed data (may take up to 3 minutes)\"\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print \"готово\"\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print \"Reading saved data...\"\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print \"done\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 20), (None, 150), (None, 67)]\n"
     ]
    }
   ],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)\n",
    "print lasagne.layers.get_output_shape([title_inp,descr_inp,cat_inp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=256)\n",
    "descr_nn = lasagne.layers.RecurrentLayer(descr_nn, num_units=128, grad_clipping=100)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=64, grad_clipping=100)\n",
    "descr_nn = lasagne.layers.DropoutLayer(descr_nn,p=0.1)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=256)\n",
    "title_nn = lasagne.layers.RecurrentLayer(title_nn, num_units=64, grad_clipping=100)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=32, grad_clipping=100)\n",
    "title_nn = lasagne.layers.DropoutLayer(title_nn,p=0.1)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, num_units=32)\n",
    "cat_nn = lasagne.layers.DropoutLayer(cat_nn,p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print lasagne.layers.get_output_shape([descr_nn, title_nn, cat_nn, lasagne.layers.ReshapeLayer(cat_nn,(([0],1,[1])))])\n",
    "nn = lasagne.layers.ConcatLayer([lasagne.layers.ReshapeLayer(descr_nn,(([0],-1))), lasagne.layers.ReshapeLayer(title_nn,(([0],-1))), lasagne.layers.ReshapeLayer(cat_nn,(([0],-1)))], axis=1)                              \n",
    "#print lasagne.layers.get_output_shape(nn)\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn,32)\n",
    "nn = lasagne.layers.BatchNormLayer(nn)\n",
    "nn = lasagne.layers.DropoutLayer(nn,p=0.25)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 0.99).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adam(loss,weights,0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 0.99).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch Train:\n",
      "\tloss: 0.797477893971\n",
      "\tacc: 0.727656765677\n",
      "\tauc: 0.803487795258\n",
      "\tap@k: 0.901157462771\n",
      "Val:\n",
      "\tloss: 0.746629713297\n",
      "\tacc: 0.559603960396\n",
      "\tauc: 0.538030594875\n",
      "\tap@k: 0.433073401477\n",
      "1 epoch Train:\n",
      "\tloss: 0.648761516039\n",
      "\tacc: 0.853861386139\n",
      "\tauc: 0.907267880795\n",
      "\tap@k: 0.991386592872\n",
      "Val:\n",
      "\tloss: 0.693257894045\n",
      "\tacc: 0.512343234323\n",
      "\tauc: 0.567535885845\n",
      "\tap@k: 0.502852301052\n",
      "2 epoch Train:\n",
      "\tloss: 0.634114934921\n",
      "\tacc: 0.865280528053\n",
      "\tauc: 0.910936381257\n",
      "\tap@k: 0.992346288823\n",
      "Val:\n",
      "\tloss: 0.689905421641\n",
      "\tacc: 0.788910891089\n",
      "\tauc: 0.829221652519\n",
      "\tap@k: 0.900049193896\n",
      "3 epoch Train:\n",
      "\tloss: 0.634960656414\n",
      "\tacc: 0.864752475248\n",
      "\tauc: 0.915421053908\n",
      "\tap@k: 0.9940096331\n",
      "Val:\n",
      "\tloss: 0.690685093027\n",
      "\tacc: 0.582376237624\n",
      "\tauc: 0.565297228744\n",
      "\tap@k: 0.488461302392\n",
      "4 epoch Train:\n",
      "\tloss: 0.624322167509\n",
      "\tacc: 0.871089108911\n",
      "\tauc: 0.928720893198\n",
      "\tap@k: 0.992758934888\n",
      "Val:\n",
      "\tloss: 0.620010015589\n",
      "\tacc: 0.707920792079\n",
      "\tauc: 0.724704211734\n",
      "\tap@k: 0.594372975874\n",
      "5 epoch Train:\n",
      "\tloss: 0.605250258065\n",
      "\tacc: 0.889438943894\n",
      "\tauc: 0.950651308879\n",
      "\tap@k: 0.987853096825\n",
      "Val:\n",
      "\tloss: 0.614545054574\n",
      "\tacc: 0.885544554455\n",
      "\tauc: 0.951223181977\n",
      "\tap@k: 0.984838684782\n",
      "6 epoch Train:\n",
      "\tloss: 0.604906258458\n",
      "\tacc: 0.888316831683\n",
      "\tauc: 0.947676558103\n",
      "\tap@k: 0.954711628325\n",
      "Val:\n",
      "\tloss: 0.613461627076\n",
      "\tacc: 0.657161716172\n",
      "\tauc: 0.659005969891\n",
      "\tap@k: 0.492555439717\n",
      "7 epoch Train:\n",
      "\tloss: 0.594030769197\n",
      "\tacc: 0.899933993399\n",
      "\tauc: 0.953500106591\n",
      "\tap@k: 0.988338189992\n",
      "Val:\n",
      "\tloss: 0.608324981928\n",
      "\tacc: 0.790561056106\n",
      "\tauc: 0.82327770676\n",
      "\tap@k: 0.954987214084\n",
      "8 epoch Train:\n",
      "\tloss: 0.593275215934\n",
      "\tacc: 0.897425742574\n",
      "\tauc: 0.954324094441\n",
      "\tap@k: 1.0\n",
      "Val:\n",
      "\tloss: 0.619838416401\n",
      "\tacc: 0.886732673267\n",
      "\tauc: 0.947412780126\n",
      "\tap@k: 0.968710701915\n",
      "9 epoch Train:\n",
      "\tloss: 0.585799014299\n",
      "\tacc: 0.903432343234\n",
      "\tauc: 0.954908287509\n",
      "\tap@k: 0.990896753488\n",
      "Val:\n",
      "\tloss: 0.598041292753\n",
      "\tacc: 0.891947194719\n",
      "\tauc: 0.927028176918\n",
      "\tap@k: 0.984440454514\n",
      "10 epoch Train:\n",
      "\tloss: 0.58819537903\n",
      "\tacc: 0.900858085809\n",
      "\tauc: 0.954865364563\n",
      "\tap@k: 0.977049024042\n",
      "Val:\n",
      "\tloss: 0.613993558239\n",
      "\tacc: 0.884752475248\n",
      "\tauc: 0.947610767369\n",
      "\tap@k: 0.970748526593\n",
      "11 epoch Train:\n",
      "\tloss: 0.592266446354\n",
      "\tacc: 0.907194719472\n",
      "\tauc: 0.958254544372\n",
      "\tap@k: 0.953772968436\n",
      "Val:\n",
      "\tloss: 0.597908428723\n",
      "\tacc: 0.904092409241\n",
      "\tauc: 0.96057825047\n",
      "\tap@k: 0.961920374451\n",
      "12 epoch Train:\n",
      "\tloss: 0.580971281308\n",
      "\tacc: 0.913201320132\n",
      "\tauc: 0.959701358656\n",
      "\tap@k: 0.958642490907\n",
      "Val:\n",
      "\tloss: 0.599440439789\n",
      "\tacc: 0.903894389439\n",
      "\tauc: 0.957521642116\n",
      "\tap@k: 0.994695606578\n",
      "13 epoch Train:\n",
      "\tloss: 0.576983657901\n",
      "\tacc: 0.913069306931\n",
      "\tauc: 0.96078492102\n",
      "\tap@k: 0.973334584781\n",
      "Val:\n",
      "\tloss: 0.603519797502\n",
      "\tacc: 0.892673267327\n",
      "\tauc: 0.929074454789\n",
      "\tap@k: 0.941059174209\n",
      "14 epoch Train:\n",
      "\tloss: 0.59489672577\n",
      "\tacc: 0.905874587459\n",
      "\tauc: 0.956877274474\n",
      "\tap@k: 0.985750308191\n",
      "Val:\n",
      "\tloss: 0.604462407198\n",
      "\tacc: 0.796171617162\n",
      "\tauc: 0.793500983505\n",
      "\tap@k: 0.950771522651\n",
      "15 epoch Train:\n",
      "\tloss: 0.583744787363\n",
      "\tacc: 0.908184818482\n",
      "\tauc: 0.956777493825\n",
      "\tap@k: 0.988220783454\n",
      "Val:\n",
      "\tloss: 0.611972182629\n",
      "\tacc: 0.895709570957\n",
      "\tauc: 0.944442535325\n",
      "\tap@k: 0.968751247896\n",
      "16 epoch Train:\n",
      "\tloss: 0.591880441966\n",
      "\tacc: 0.910297029703\n",
      "\tauc: 0.955693279856\n",
      "\tap@k: 0.988572124483\n",
      "Val:\n",
      "\tloss: 0.594955474301\n",
      "\tacc: 0.902310231023\n",
      "\tauc: 0.956519895604\n",
      "\tap@k: 0.969010661033\n",
      "17 epoch Train:\n",
      "\tloss: 0.584428722268\n",
      "\tacc: 0.910099009901\n",
      "\tauc: 0.953344190944\n",
      "\tap@k: 0.963586276597\n",
      "Val:\n",
      "\tloss: 0.596365626239\n",
      "\tacc: 0.753267326733\n",
      "\tauc: 0.714694886028\n",
      "\tap@k: 0.953331813936\n",
      "18 epoch Train:\n",
      "\tloss: 0.581234159808\n",
      "\tacc: 0.910297029703\n",
      "\tauc: 0.958578498441\n",
      "\tap@k: 0.979549532204\n",
      "Val:\n",
      "\tloss: 0.596551496425\n",
      "\tacc: 0.898481848185\n",
      "\tauc: 0.946989613028\n",
      "\tap@k: 0.966862489666\n",
      "19 epoch Train:\n",
      "\tloss: 0.571414840505\n",
      "\tacc: 0.921188118812\n",
      "\tauc: 0.960025166797\n",
      "\tap@k: 0.972900268225\n",
      "Val:\n",
      "\tloss: 0.59758439738\n",
      "\tacc: 0.774125412541\n",
      "\tauc: 0.754307223575\n",
      "\tap@k: 0.944123373058\n",
      "20 epoch Train:\n",
      "\tloss: 0.579580682995\n",
      "\tacc: 0.914851485149\n",
      "\tauc: 0.959847876431\n",
      "\tap@k: 0.973101838169\n",
      "Val:\n",
      "\tloss: 0.601085022059\n",
      "\tacc: 0.899801980198\n",
      "\tauc: 0.952898863231\n",
      "\tap@k: 0.99511515372\n",
      "21 epoch Train:\n",
      "\tloss: 0.576712093993\n",
      "\tacc: 0.919801980198\n",
      "\tauc: 0.962204412937\n",
      "\tap@k: 0.975772871363\n",
      "Val:\n",
      "\tloss: 0.59412049667\n",
      "\tacc: 0.903432343234\n",
      "\tauc: 0.956094288771\n",
      "\tap@k: 0.93772187629\n",
      "22 epoch Train:\n",
      "\tloss: 0.576960260998\n",
      "\tacc: 0.919339933993\n",
      "\tauc: 0.962162639488\n",
      "\tap@k: 0.988943138437\n",
      "Val:\n",
      "\tloss: 0.601000668057\n",
      "\tacc: 0.903498349835\n",
      "\tauc: 0.953578534281\n",
      "\tap@k: 0.96383085\n",
      "23 epoch Train:\n",
      "\tloss: 0.578976364744\n",
      "\tacc: 0.912673267327\n",
      "\tauc: 0.957853167402\n",
      "\tap@k: 0.983620127991\n",
      "Val:\n",
      "\tloss: 0.591150537915\n",
      "\tacc: 0.906534653465\n",
      "\tauc: 0.955295028332\n",
      "\tap@k: 0.978245402676\n",
      "24 epoch Train:\n",
      "\tloss: 0.57423758354\n",
      "\tacc: 0.915775577558\n",
      "\tauc: 0.959215389978\n",
      "\tap@k: 0.977594625893\n",
      "Val:\n",
      "\tloss: 0.594439177857\n",
      "\tacc: 0.905478547855\n",
      "\tauc: 0.956379983801\n",
      "\tap@k: 0.969804486762\n",
      "25 epoch Train:\n",
      "\tloss: 0.598810370693\n",
      "\tacc: 0.893267326733\n",
      "\tauc: 0.944536414477\n",
      "\tap@k: 0.947167685593\n",
      "Val:\n",
      "\tloss: 0.59279318138\n",
      "\tacc: 0.498349834983\n",
      "\tauc: 0.499932554955\n",
      "\tap@k: 0.526927114924\n",
      "26 epoch Train:\n",
      "\tloss: 0.58398792156\n",
      "\tacc: 0.912871287129\n",
      "\tauc: 0.956689254766\n",
      "\tap@k: 0.94831190846\n",
      "Val:\n",
      "\tloss: 0.586344481904\n",
      "\tacc: 0.771881188119\n",
      "\tauc: 0.802212085339\n",
      "\tap@k: 0.932840491138\n",
      "27 epoch Train:\n",
      "\tloss: 0.570066046377\n",
      "\tacc: 0.922112211221\n",
      "\tauc: 0.963936560794\n",
      "\tap@k: 0.984154208851\n",
      "Val:\n",
      "\tloss: 0.581436780092\n",
      "\tacc: 0.616765676568\n",
      "\tauc: 0.629138335698\n",
      "\tap@k: 0.456468190782\n",
      "28 epoch Train:\n",
      "\tloss: 0.571969312742\n",
      "\tacc: 0.922178217822\n",
      "\tauc: 0.965955673059\n",
      "\tap@k: 0.966303459013\n",
      "Val:\n",
      "\tloss: 0.585634814383\n",
      "\tacc: 0.900660066007\n",
      "\tauc: 0.938319119895\n",
      "\tap@k: 0.952672219224\n",
      "29 epoch Train:\n",
      "\tloss: 0.562319952607\n",
      "\tacc: 0.92396039604\n",
      "\tauc: 0.962586101149\n",
      "\tap@k: 0.961149946563\n",
      "Val:\n",
      "\tloss: 0.590673239455\n",
      "\tacc: 0.775511551155\n",
      "\tauc: 0.80444172278\n",
      "\tap@k: 0.943640308462\n",
      "30 epoch Train:\n",
      "\tloss: 0.57297118412\n",
      "\tacc: 0.922178217822\n",
      "\tauc: 0.96597844264\n",
      "\tap@k: 0.97726506764\n",
      "Val:\n",
      "\tloss: 0.597109270639\n",
      "\tacc: 0.777293729373\n",
      "\tauc: 0.803233113356\n",
      "\tap@k: 0.953225752503\n",
      "31 epoch Train:\n",
      "\tloss: 0.574463648657\n",
      "\tacc: 0.920066006601\n",
      "\tauc: 0.961995118245\n",
      "\tap@k: 0.986444860437\n",
      "Val:\n",
      "\tloss: 0.592189615168\n",
      "\tacc: 0.903366336634\n",
      "\tauc: 0.95649080826\n",
      "\tap@k: 0.970508846912\n",
      "32 epoch Train:\n",
      "\tloss: 0.567782135706\n",
      "\tacc: 0.926732673267\n",
      "\tauc: 0.964541510719\n",
      "\tap@k: 0.968336499845\n",
      "Val:\n",
      "\tloss: 0.585920975737\n",
      "\tacc: 0.912145214521\n",
      "\tauc: 0.958688634904\n",
      "\tap@k: 0.954311175549\n",
      "33 epoch Train:\n",
      "\tloss: 0.560587798316\n",
      "\tacc: 0.928580858086\n",
      "\tauc: 0.965665904433\n",
      "\tap@k: 0.945448036595\n",
      "Val:\n",
      "\tloss: 0.594994471558\n",
      "\tacc: 0.911617161716\n",
      "\tauc: 0.954691393761\n",
      "\tap@k: 0.981916642864\n",
      "34 epoch Train:\n",
      "\tloss: 0.573591517678\n",
      "\tacc: 0.923300330033\n",
      "\tauc: 0.959608382712\n",
      "\tap@k: 0.954178179063\n",
      "Val:\n",
      "\tloss: 0.5882389581\n",
      "\tacc: 0.88396039604\n",
      "\tauc: 0.931727496676\n",
      "\tap@k: 0.969277349477\n",
      "35 epoch Train:\n",
      "\tloss: 0.574030779781\n",
      "\tacc: 0.921386138614\n",
      "\tauc: 0.961747534566\n",
      "\tap@k: 0.945352046763\n",
      "Val:\n",
      "\tloss: 0.595301973309\n",
      "\tacc: 0.846204620462\n",
      "\tauc: 0.885355905054\n",
      "\tap@k: 0.966779635613\n",
      "36 epoch Train:\n",
      "\tloss: 0.573515330343\n",
      "\tacc: 0.919339933993\n",
      "\tauc: 0.960177716946\n",
      "\tap@k: 0.97288105517\n",
      "Val:\n",
      "\tloss: 0.602911436156\n",
      "\tacc: 0.897227722772\n",
      "\tauc: 0.949201215737\n",
      "\tap@k: 0.95686003528\n",
      "37 epoch Train:\n",
      "\tloss: 0.573152939281\n",
      "\tacc: 0.92\n",
      "\tauc: 0.959158846211\n",
      "\tap@k: 0.99004314635\n",
      "Val:\n",
      "\tloss: 0.596120635483\n",
      "\tacc: 0.906864686469\n",
      "\tauc: 0.954270697785\n",
      "\tap@k: 0.963831180894\n",
      "38 epoch Train:\n",
      "\tloss: 0.571154490418\n",
      "\tacc: 0.921056105611\n",
      "\tauc: 0.96075003127\n",
      "\tap@k: 0.958495101747\n",
      "Val:\n",
      "\tloss: 0.586786931877\n",
      "\tacc: 0.897557755776\n",
      "\tauc: 0.949376538059\n",
      "\tap@k: 0.973514185537\n",
      "39 epoch Train:\n",
      "\tloss: 0.572087140725\n",
      "\tacc: 0.922112211221\n",
      "\tauc: 0.96299298892\n",
      "\tap@k: 0.932427577346\n",
      "Val:\n",
      "\tloss: 0.596989634913\n",
      "\tacc: 0.901452145215\n",
      "\tauc: 0.950425830549\n",
      "\tap@k: 0.96961741183\n",
      "40 epoch Train:\n",
      "\tloss: 0.573839181102\n",
      "\tacc: 0.91900990099\n",
      "\tauc: 0.962087117651\n",
      "\tap@k: 0.987948206876\n",
      "Val:\n",
      "\tloss: 0.600852040279\n",
      "\tacc: 0.90495049505\n",
      "\tauc: 0.954501870764\n",
      "\tap@k: 0.982185836264\n",
      "41 epoch Train:\n",
      "\tloss: 0.574385226623\n",
      "\tacc: 0.923432343234\n",
      "\tauc: 0.96404120392\n",
      "\tap@k: 0.970584351857\n",
      "Val:\n",
      "\tloss: 0.587670208329\n",
      "\tacc: 0.908514851485\n",
      "\tauc: 0.955148517868\n",
      "\tap@k: 0.965625054022\n",
      "42 epoch Train:\n",
      "\tloss: 0.579420903532\n",
      "\tacc: 0.914125412541\n",
      "\tauc: 0.956975970889\n",
      "\tap@k: 0.963487511887\n",
      "Val:\n",
      "\tloss: 0.595862924685\n",
      "\tacc: 0.609768976898\n",
      "\tauc: 0.565455732774\n",
      "\tap@k: 0.94601726751\n",
      "43 epoch Train:\n",
      "\tloss: 0.571820216878\n",
      "\tacc: 0.918745874587\n",
      "\tauc: 0.962028058937\n",
      "\tap@k: 0.975251876542\n",
      "Val:\n",
      "\tloss: 0.592731311874\n",
      "\tacc: 0.901584158416\n",
      "\tauc: 0.949224678229\n",
      "\tap@k: 0.917028135343\n",
      "44 epoch Train:\n",
      "\tloss: 0.569918028385\n",
      "\tacc: 0.922574257426\n",
      "\tauc: 0.961575445109\n",
      "\tap@k: 0.963073617103\n",
      "Val:\n",
      "\tloss: 0.589231951048\n",
      "\tacc: 0.90904290429\n",
      "\tauc: 0.954433578172\n",
      "\tap@k: 0.920664011019\n",
      "45 epoch Train:\n",
      "\tloss: 0.572495223002\n",
      "\tacc: 0.921716171617\n",
      "\tauc: 0.962470027941\n",
      "\tap@k: 0.958730016806\n",
      "Val:\n",
      "\tloss: 0.599124901825\n",
      "\tacc: 0.893663366337\n",
      "\tauc: 0.944925899782\n",
      "\tap@k: 0.953519581196\n",
      "46 epoch Train:\n",
      "\tloss: 0.577051037805\n",
      "\tacc: 0.918613861386\n",
      "\tauc: 0.960429155437\n",
      "\tap@k: 0.951632071554\n",
      "Val:\n",
      "\tloss: 0.601063656542\n",
      "\tacc: 0.901386138614\n",
      "\tauc: 0.950023216153\n",
      "\tap@k: 0.96769918426\n",
      "47 epoch Train:\n",
      "\tloss: 0.574915530459\n",
      "\tacc: 0.916369636964\n",
      "\tauc: 0.95849140648\n",
      "\tap@k: 0.983822840974\n",
      "Val:\n",
      "\tloss: 0.596363041257\n",
      "\tacc: 0.806072607261\n",
      "\tauc: 0.847007734902\n",
      "\tap@k: 0.945399301788\n",
      "48 epoch Train:\n",
      "\tloss: 0.575357208713\n",
      "\tacc: 0.916699669967\n",
      "\tauc: 0.958936579757\n",
      "\tap@k: 0.994806195939\n",
      "Val:\n",
      "\tloss: 0.588946632499\n",
      "\tacc: 0.894587458746\n",
      "\tauc: 0.950343078135\n",
      "\tap@k: 0.963399193156\n",
      "49 epoch Train:\n",
      "\tloss: 0.576388805316\n",
      "\tacc: 0.919273927393\n",
      "\tauc: 0.960805277482\n",
      "\tap@k: 0.975539638436\n",
      "Val:\n",
      "\tloss: 0.598532243291\n",
      "\tacc: 0.905214521452\n",
      "\tauc: 0.955459784614\n",
      "\tap@k: 0.964446999251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 150\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print i, \"epoch Train:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.5)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print \"Val:\"\n",
    "    print '\\tloss:',b_loss/b_c\n",
    "    print '\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.5)\n",
    "    print '\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "    print '\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print \"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.595950740976\n",
      "\tacc: 0.903283310596\n",
      "\tauc: 0.953475533461\n",
      "\tap@k: 0.958163560258\n",
      "\n",
      "AUC:\n",
      "\tСойдёт, хотя можно ещё поднажать (ok)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tОтличный результат (good)\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0.5)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.55078459e-01   9.58299870e-09   9.98377382e-01 ...,   3.72263514e-07\n",
      "   9.86608296e-01   9.99957971e-01] [1 0 1 ..., 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print epoch_y_pred, epoch_y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
